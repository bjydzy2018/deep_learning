### 第二周：机器学习策略（二）

#### 进行错误分析
当模型准确度还未达到人类水平时，表明模型还有提升的空间，而模型应该往哪些方面进行改进是效率最高的？提升空间最大的？可以通过误差分析来获得思路。这时候切忌盲目去做去改进，要在误差分析基础上去提升。

误差分析就是从识别错误的样本中随机选择若干个样本如100个，然后将错误分类的类型进行统计，可以根据错误类型的占比给出优先级，对于那些占比高的优先级高，优先集中精力去解决，对于最终结果的提升效率最高，可以提升的空间最大。

#### 清除错误标记的样本
情况一：

深度学习算法对训练集中的随机误差具有相当的鲁棒性。

只要我们标记出错的例子符合随机误差，如：做标记的人不小心错误，或按错分类键。那么像这种随机误差导致的标记错误，一般来说不管这些误差可能也没有问题。

所以对于这类误差，我们可以不去用大量的时间和精力去做修正，只要数据集足够大，实际误差不会因为这些随机误差有很大的变化。

情况二：

虽然深度学习算法对随机误差具有很好的鲁棒性，但是对于系统误差就不是这样了。

如果做标记的人一直把如例子中的白色的狗标记成猫，那么最终导致我们的分类器就会出现错误。

修正开发、测试集上错误样例：

对开发集和测试集上的数据进行检查，确保他们来自于相同的分布。使得我们以开发集为目标方向，更正确地将算法应用到测试集上。
考虑算法分类错误的样本的同时也去考虑算法分类正确的样本。（通常难度比较大，很少这么做，因为分类正确的是大多数，而分类错误的仅占很小一部分，容易去分析）
训练集和开发/测试集可能来自不同的分布。

#### 快速搭建第一个模型，并迅速迭代
当你在一个新的领域准备构建一个机器学习项目时，Andrew建议是首先建立一个baseline的模型，然后通过偏差、方差分析以及误差分析等方法，确定下一步要优化的方向和优先级。

总之就是：快速的建立自己的基本系统，并进行迭代。而不是想的太多，在一开始就建立一个非常复杂，难以入手的系统，当然你在某个领域内有丰富经验的除外。

#### 在不同分布上的训练和测试
在深度学习的时代，使用的训练集数据大部分是和开发集和测试集来自不同的分布。对于这些不同分布的数据，应该如何处理呢？

Andrew从猫分类问题为例进行了说明：
猫分类问题中的训练集数据来自互联网爬虫，图片分辨率高，清晰度高，数据量比较多；而对于用户从app上上传的图片清晰度低，而且数量少；对于后者是我们需要的数据，是模型上线后要分类的图片。这时候训练集数据和开发集、测试集来自不同的分布。
![](different_distribution.png)
面对这种问题正确的做法是：如图所示
训练集均是来自网上下载的20万张高清图片，当然也可以加上5000张手机非高清图片；对于开发和测试集都是手机非高清图片。把用户app上传的一部分数据放到训练集中。
开发和测试集合全部是手机app上传的数据，也就是以后模型上线后要分类的数据。

好处：开发集全部来自手机图片，瞄准目标；
坏处：训练集和开发、测试集来自不同的分布。
从长期来看，这样的分布能够给我们带来更好的系统性能。

#### 不同分布上的偏差和方差
**引出问题**
传统的如果训练集和开发、测试集是同一个分布情况,如下图：
分析训练误差以及开发误差，能够清晰界定是偏差问题或者方差问题。
但当训练集和开发、测试集分布不同时，当有下图的结果，就不能简单的下结论了，因为有两种可能导致下面问题：首先算法只见过训练集数据，没见过开发集数据。第二，开发集数据来自不同的分布。
![](data_mismatch.png)

**解决问题**
新引入一个数据集合，称为train-dev数据集，这个数据通过在训练集中随机抽取得到，但是不用来训练模型。这个数据保证了与训练集合是同一个数据分布。
这样就形成了下面的数据分类:开发集和测试集来自同一分布，训练集和训练-开发集也来自同一分布。
![](train_dev_set.png)

如果最终，我们的模型得到的误差分别为：

Training error： 1\%
Training-dev error： 9\%
Dev error： 10\%
那么，由于训练开发集尽管和训练集来自同一分布，但是却有很大的误差， 模型无法泛化到同分布的数据，那么说明我们的模型存在**方差问题**。

但如果我们的模型得到的误差分别为：

Training error： 1\%
Training-dev error： 1.5\%
Dev error： 10\%
那么在这样的情况下，我们可以看到，来自同分布的数据，模型的泛化能力强，而开发集的误差主要是来自于**分布不匹配**导致的。

**分布不同的偏差方差分析**：
通过：Human level、Training set error、Training-dev set error、Dev error、Test error 之间误差的大小，可以分别得知我们的模型，需要依次在：可避免的偏差、方差、数据分布不匹配、开发集的或拟合程度，这些方面做改进。
![](bias_var_mismatched.png)


#### 解决数据分布不匹配问题
对于数据分布不匹配问题，没有系统的解决方案。Andrew给出了一些指导原则：
进行人工误差分析，尝试去了解训练集和开发测试集的具体差异在哪里。如：噪音等；
尝试把训练数据变得更像开发集，或者收集更多的类似开发集和测试集的数据，做法是人工数据合成。

#### 迁移学习
深度学习中，最强大的理念之一就是，有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中。Andrew在课中举了两个例子，如下图：
![](transfer_learning.png)

第一个例子是图像识别的例子，应用到医学放射扫描图像。你可以做的是把图像识别形成的神经网络最后的输出层拿走，就把它删掉，还有进入到最后一层的权重删掉，然后为最后一层重新赋予随机权重，然后让它在放射诊断数据上进行训练。把图像识别中学到的知识应用或迁移到放射科诊断上来，为什么这样做有效果呢？有很多低层次特征，比如说边缘检测、曲线检测等，从非常大的图像识别数据库中习得这些能力可能有助于你的学习算法在放射科诊断中做得更好。
**迁移学习的应用场景**：
- 迁移学习起作用的场合是，在迁移来源问题中你有很多数据，但迁移目标问题你没有那么多数据
- 可以学习低层次特征，可以在神经网络的前面几层学到如何识别很多有用的特征；

所以总结一下，什么时候迁移学习是有意义的？
- 任务A和任务B都有同样的输入x时，迁移学习是有意义的。在第一个例子中，A和B的输入都是图像。
- 当任务A的数据比任务B多得多时，迁移学习意义更大。
- 习得任务A的低层次特征，可以帮助任务B的学习，那迁移学习更有意义一些

#### 多任务学习
相比于迁移学习的串行学习，多任务学习是并行学习，是训练一个单独的神经网络同时完成多个任务。相对于迁移学习，多任务学习的应用场景要少一些，但在计算机视觉中的物体检测中应用较多。

视频中的一个例子是自动驾驶中需要同时识别图像中的行人、汽车、标志、交通灯等，是一个多任务学习应该场景。
**Notes**：
单个神经网络的多任务学习也可以转为训练多个神经网络的单任务学习：多任务学习会降低性能的唯一情况就是你的神经网络还不够大。

![](multitask_learning.png)
第一，如果你训练的一组任务，可以共用低层次特征。对于无人驾驶的例子，同时识别交通灯、汽车和行人是有道理的，这些物体有相似的特征，也许能帮你识别停车标志，因为这些都是道路上的特征。
第二，每个任务中的数据量很相近；（如，在迁移学习中由任务A“100万数据”迁移到任务B“1000数据”；多任务学习中，任务$ A_{1},...,A_{n}$ ，每个任务均有1000个数据，合起来就有1000n个数据，共同帮助任务的训练，提升训练集的规模）
第三，可以训练一个足够大的神经网络并同时做好所有的任务。

#### 什么是端到端深度学习
端到端深度学习就是省去传统机器学习过程的中间处理环节，输入数据直接给出结果。

端到端深度学习应用场景就是需要海量的数据。如果没有海量的数据，还是传统的方法更好。

在少数据集的情况下传统的特征提取方式可能会取得好的效果；如果在有足够的大量数据集情况下，端到端的深度学习会发挥巨大的价值。

提到了人脸识别的案例：目前在没有海量数据的情况下，两步法的效果更好。把人脸识别问题分解成两个更简单的步骤：首先，是弄清楚脸在哪里。第二步是看着脸，弄清楚这是谁。这第二种方法让学习算法，或者说两个学习算法分别解决两个更简单的任务，并在整体上得到更好的表现。

优缺点：

优点：
端到端学习可以直接让数据“说话”；
所需手工设计的组件更少，中间环节更少。
缺点：
需要大量的数据；
排除了可能有用的手工设计组件

